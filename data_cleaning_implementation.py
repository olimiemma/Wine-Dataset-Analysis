#!/usr/bin/env python3
"""
Comprehensive Data Cleaning Implementation Pipeline
Implements all transformations from the data preparation strategy.
Includes validation, before/after comparisons, and detailed logging.
"""

import pandas as pd
import numpy as np
from scipy import stats
from scipy.stats import shapiro, normaltest, jarque_bera, anderson
from sklearn.preprocessing import StandardScaler, RobustScaler, MinMaxScaler
from sklearn.preprocessing import PowerTransformer, QuantileTransformer
from sklearn.impute import SimpleImputer
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
from datetime import datetime
import os
import json

warnings.filterwarnings('ignore')

class ComprehensiveDataCleaner:
    """
    Complete data cleaning pipeline implementation.
    Executes all transformations from the preparation strategy.
    """
    
    def __init__(self, df, output_dir='data_cleaning_results'):
        """Initialize with dataset and output directory."""
        self.original_df = df.copy()
        self.df = df.copy()
        self.output_dir = output_dir
        self.transformation_log = []
        self.validation_results = {}
        self.scalers = {}
        
        # Create output directory
        os.makedirs(output_dir, exist_ok=True)
        os.makedirs(f"{output_dir}/plots", exist_ok=True)
        
        print(f"üîß Data Cleaning Pipeline Initialized")
        print(f"üìä Original dataset: {df.shape[0]:,} rows √ó {df.shape[1]} columns")
        print(f"üìÅ Output directory: {output_dir}")
        
    def log_transformation(self, step, description, details=None):
        """Log transformation step with timestamp."""
        log_entry = {
            'timestamp': datetime.now().isoformat(),
            'step': step,
            'description': description,
            'details': details or {},
            'data_shape': self.df.shape
        }
        self.transformation_log.append(log_entry)
        print(f"‚úÖ {step}: {description}")
    
    def apply_missing_data_treatment(self):
        """
        Apply comprehensive missing data treatment based on strategy.
        
        Strategy:
        - STARS: Median imputation (26.3% missing, strongest predictor)
        - Low missing rate variables (<10%): Mean/Median imputation
        """
        print("\nüéØ PHASE 1: MISSING DATA TREATMENT")
        print("=" * 50)
        
        missing_before = self.df.isnull().sum()
        total_missing_before = missing_before.sum()
        
        # Define imputation strategies based on strategy document
        mean_impute_vars = ['ResidualSugar', 'Chlorides', 'pH']
        median_impute_vars = ['FreeSulfurDioxide', 'TotalSulfurDioxide', 'Sulphates', 'Alcohol', 'STARS']
        
        # Mean imputation for low missing rate variables
        for var in mean_impute_vars:
            if var in self.df.columns and self.df[var].isnull().any():
                missing_count = self.df[var].isnull().sum()
                mean_value = self.df[var].mean()
                self.df[var].fillna(mean_value, inplace=True)
                
                self.log_transformation(
                    f"Mean Imputation - {var}",
                    f"Filled {missing_count:,} missing values with mean {mean_value:.3f}",
                    {'method': 'mean', 'imputed_value': mean_value, 'missing_count': missing_count}
                )
        
        # Median imputation for skewed variables and STARS
        for var in median_impute_vars:
            if var in self.df.columns and self.df[var].isnull().any():
                missing_count = self.df[var].isnull().sum()
                median_value = self.df[var].median()
                self.df[var].fillna(median_value, inplace=True)
                
                if var == 'STARS':
                    self.log_transformation(
                        f"üåü CRITICAL: Median Imputation - {var}",
                        f"Filled {missing_count:,} missing values with median {median_value:.1f} (strongest sales predictor)",
                        {'method': 'median', 'imputed_value': median_value, 'missing_count': missing_count}
                    )
                else:
                    self.log_transformation(
                        f"Median Imputation - {var}",
                        f"Filled {missing_count:,} missing values with median {median_value:.3f}",
                        {'method': 'median', 'imputed_value': median_value, 'missing_count': missing_count}
                    )
        
        missing_after = self.df.isnull().sum()
        total_missing_after = missing_after.sum()
        
        print(f"üìà Missing data reduction: {total_missing_before:,} ‚Üí {total_missing_after:,}")
        print(f"üìä Data utilization improvement: {(total_missing_before - total_missing_after) / len(self.df) * 100:.1f}%")
        
        # Validate STARS imputation preserved correlation with TARGET
        if 'STARS' in self.df.columns and 'TARGET' in self.df.columns:
            correlation = self.df['STARS'].corr(self.df['TARGET'])
            print(f"üåü STARS-TARGET correlation after imputation: {correlation:.3f}")
            
        return self.df.copy()
    
    def handle_outliers_comprehensive(self):
        """
        Apply tiered outlier treatment framework.
        
        Strategy:
        - Tier 1 (Severe): Winsorization at 1st/99th percentiles
        - Tier 2 (Moderate): Robust scaling preparation
        - Tier 3 (Mild): Light treatment
        """
        print("\n‚ö†Ô∏è  PHASE 2: OUTLIER TREATMENT FRAMEWORK")
        print("=" * 50)
        
        # Exclude categorical and target variables from outlier treatment
        exclude_vars = ['TARGET', 'STARS']  # STARS treated as categorical
        numeric_vars = [col for col in self.df.select_dtypes(include=[np.number]).columns 
                       if col not in exclude_vars]
        
        outlier_summary = {}
        
        for var in numeric_vars:
            if var not in self.df.columns:
                continue
                
            data = self.df[var].dropna()
            if len(data) == 0:
                continue
                
            # Calculate outlier bounds using IQR method
            Q1 = data.quantile(0.25)
            Q3 = data.quantile(0.75)
            IQR = Q3 - Q1
            
            # Severe outliers (beyond 3*IQR)
            severe_lower = Q1 - 3 * IQR
            severe_upper = Q3 + 3 * IQR
            
            # Moderate outliers (beyond 1.5*IQR but within 3*IQR)
            moderate_lower = Q1 - 1.5 * IQR
            moderate_upper = Q3 + 1.5 * IQR
            
            severe_mask = (data < severe_lower) | (data > severe_upper)
            moderate_mask = ((data < moderate_lower) | (data > moderate_upper)) & ~severe_mask
            
            severe_count = severe_mask.sum()
            moderate_count = moderate_mask.sum()
            total_count = len(data)
            
            outlier_summary[var] = {
                'severe_count': severe_count,
                'moderate_count': moderate_count,
                'severe_pct': severe_count / total_count * 100,
                'moderate_pct': moderate_count / total_count * 100
            }
            
            # Apply Tier 1: Winsorization for severe outliers
            if severe_count > 0:
                # Winsorize at 1st and 99th percentiles
                p01 = data.quantile(0.01)
                p99 = data.quantile(0.99)
                
                self.df[var] = self.df[var].clip(lower=p01, upper=p99)
                
                self.log_transformation(
                    f"Tier 1 Outlier Treatment - {var}",
                    f"Winsorized {severe_count:,} severe outliers ({severe_count/total_count*100:.1f}%)",
                    {
                        'method': 'winsorization',
                        'lower_bound': p01,
                        'upper_bound': p99,
                        'outliers_treated': severe_count
                    }
                )
        
        total_severe = sum([info['severe_count'] for info in outlier_summary.values()])
        total_moderate = sum([info['moderate_count'] for info in outlier_summary.values()])
        total_observations = len(self.df) * len(numeric_vars)
        
        print(f"üéØ Outlier treatment summary:")
        print(f"   ‚Ä¢ Severe outliers winsorized: {total_severe:,} ({total_severe/total_observations*100:.1f}%)")
        print(f"   ‚Ä¢ Moderate outliers flagged: {total_moderate:,} ({total_moderate/total_observations*100:.1f}%)")
        print(f"   ‚Ä¢ Variables treated: {len([v for v in outlier_summary.values() if v['severe_count'] > 0])}")
        
        return self.df.copy()
    
    def transform_distributions(self):
        """
        Apply distribution transformations based on strategy.
        
        Strategy:
        - Yeo-Johnson: Variables with negative values
        - Quantile Transform: Complex distributions
        - Box-Cox: Right-skewed positive variables
        - No Transform: TARGET, STARS
        """
        print("\nüìä PHASE 3: DISTRIBUTION TRANSFORMATION PIPELINE")
        print("=" * 50)
        
        # Define transformation strategies based on strategy document
        no_transform_vars = ['TARGET', 'STARS']
        yeo_johnson_vars = ['FixedAcidity', 'VolatileAcidity', 'CitricAcid', 'ResidualSugar', 
                           'Chlorides', 'FreeSulfurDioxide', 'TotalSulfurDioxide', 
                           'Sulphates', 'Alcohol', 'LabelAppeal']
        quantile_vars = ['Density', 'pH']
        box_cox_vars = ['AcidIndex']
        
        # Store transformers for inverse transformation if needed
        self.transformers = {}
        
        # Apply Yeo-Johnson transformation
        for var in yeo_johnson_vars:
            if var not in self.df.columns:
                continue
                
            # Check for negative values
            has_negative = (self.df[var] < 0).any()
            
            transformer = PowerTransformer(method='yeo-johnson', standardize=False)
            
            # Fit and transform
            original_data = self.df[var].values.reshape(-1, 1)
            transformed_data = transformer.fit_transform(original_data)
            self.df[var] = transformed_data.flatten()
            
            # Store transformer
            self.transformers[var] = transformer
            
            # Calculate improvement metrics
            original_skew = stats.skew(original_data.flatten())
            transformed_skew = stats.skew(transformed_data.flatten())
            
            self.log_transformation(
                f"Yeo-Johnson Transform - {var}",
                f"Transformed distribution (skewness: {original_skew:.3f} ‚Üí {transformed_skew:.3f})",
                {
                    'method': 'yeo-johnson',
                    'original_skewness': original_skew,
                    'transformed_skewness': transformed_skew,
                    'has_negative_values': has_negative
                }
            )
        
        # Apply Quantile transformation
        for var in quantile_vars:
            if var not in self.df.columns:
                continue
                
            transformer = QuantileTransformer(output_distribution='normal', random_state=42)
            
            original_data = self.df[var].values.reshape(-1, 1)
            transformed_data = transformer.fit_transform(original_data)
            self.df[var] = transformed_data.flatten()
            
            # Store transformer
            self.transformers[var] = transformer
            
            self.log_transformation(
                f"Quantile Transform - {var}",
                f"Applied quantile transformation to normal distribution",
                {
                    'method': 'quantile_normal',
                    'output_distribution': 'normal'
                }
            )
        
        # Apply Box-Cox transformation
        for var in box_cox_vars:
            if var not in self.df.columns:
                continue
                
            # Ensure positive values for Box-Cox
            min_val = self.df[var].min()
            if min_val <= 0:
                shift = abs(min_val) + 1
                self.df[var] = self.df[var] + shift
                print(f"   Shifted {var} by {shift:.3f} for Box-Cox compatibility")
            
            transformer = PowerTransformer(method='box-cox', standardize=False)
            
            original_data = self.df[var].values.reshape(-1, 1)
            transformed_data = transformer.fit_transform(original_data)
            self.df[var] = transformed_data.flatten()
            
            # Store transformer
            self.transformers[var] = transformer
            
            # Calculate improvement metrics
            original_skew = stats.skew(original_data.flatten())
            transformed_skew = stats.skew(transformed_data.flatten())
            
            self.log_transformation(
                f"Box-Cox Transform - {var}",
                f"Transformed right-skewed distribution (skewness: {original_skew:.3f} ‚Üí {transformed_skew:.3f})",
                {
                    'method': 'box-cox',
                    'original_skewness': original_skew,
                    'transformed_skewness': transformed_skew
                }
            )
        
        print(f"üìà Distribution transformations completed:")
        print(f"   ‚Ä¢ Yeo-Johnson: {len([v for v in yeo_johnson_vars if v in self.df.columns])} variables")
        print(f"   ‚Ä¢ Quantile: {len([v for v in quantile_vars if v in self.df.columns])} variables")
        print(f"   ‚Ä¢ Box-Cox: {len([v for v in box_cox_vars if v in self.df.columns])} variables")
        print(f"   ‚Ä¢ Preserved: {len([v for v in no_transform_vars if v in self.df.columns])} variables")
        
        return self.df.copy()
    
    def create_engineered_features(self):
        """
        Engineer wine chemistry and quality interaction features.
        
        Strategy:
        - Chemistry ratios: SO2, acidity, sugar-alcohol relationships
        - Quality-marketing interactions: STARS √ó LabelAppeal synergy
        """
        print("\nüî¨ PHASE 4: FEATURE ENGINEERING OPPORTUNITIES")
        print("=" * 50)
        
        features_created = 0
        
        # 1. Chemistry Ratio Features
        print("Creating chemistry ratio features...")
        
        # Total to Free SO2 ratio
        if 'TotalSulfurDioxide' in self.df.columns and 'FreeSulfurDioxide' in self.df.columns:
            self.df['total_to_free_so2_ratio'] = (
                self.df['TotalSulfurDioxide'] / (self.df['FreeSulfurDioxide'] + 1)
            )
            features_created += 1
            self.log_transformation(
                "Feature Engineering - SO2 Ratio",
                "Created total_to_free_so2_ratio (SO2 binding effectiveness)",
                {'formula': 'TotalSulfurDioxide / (FreeSulfurDioxide + 1)'}
            )
        
        # Acidity balance ratio
        if 'FixedAcidity' in self.df.columns and 'VolatileAcidity' in self.df.columns:
            self.df['acidity_balance_ratio'] = (
                self.df['FixedAcidity'] / (self.df['VolatileAcidity'] + 0.1)
            )
            features_created += 1
            self.log_transformation(
                "Feature Engineering - Acidity Balance",
                "Created acidity_balance_ratio (good vs bad acidity)",
                {'formula': 'FixedAcidity / (VolatileAcidity + 0.1)'}
            )
        
        # Sugar-alcohol ratio
        if 'ResidualSugar' in self.df.columns and 'Alcohol' in self.df.columns:
            self.df['sugar_alcohol_ratio'] = (
                self.df['ResidualSugar'] / (self.df['Alcohol'] + 1)
            )
            features_created += 1
            self.log_transformation(
                "Feature Engineering - Sugar-Alcohol",
                "Created sugar_alcohol_ratio (dryness indicator)",
                {'formula': 'ResidualSugar / (Alcohol + 1)'}
            )
        
        # Density-alcohol deviation
        if 'Density' in self.df.columns and 'Alcohol' in self.df.columns:
            expected_density = 1.0 - 0.01 * self.df['Alcohol']
            self.df['density_alcohol_deviation'] = np.abs(self.df['Density'] - expected_density)
            features_created += 1
            self.log_transformation(
                "Feature Engineering - Density Deviation",
                "Created density_alcohol_deviation (authenticity indicator)",
                {'formula': 'abs(Density - (1.0 - 0.01 * Alcohol))'}
            )
        
        # 2. Quality-Marketing Interactions
        print("Creating quality-marketing interaction features...")
        
        # STARS √ó LabelAppeal synergy
        if 'STARS' in self.df.columns and 'LabelAppeal' in self.df.columns:
            self.df['stars_label_synergy'] = self.df['STARS'] * self.df['LabelAppeal']
            features_created += 1
            self.log_transformation(
                "Feature Engineering - Premium Synergy",
                "Created stars_label_synergy (top 2 sales predictors interaction)",
                {'formula': 'STARS * LabelAppeal'}
            )
        
        # Quality-chemistry interaction
        if 'STARS' in self.df.columns and 'AcidIndex' in self.df.columns:
            # Normalize AcidIndex to 0-1 scale first
            acid_normalized = (self.df['AcidIndex'] - self.df['AcidIndex'].min()) / (
                self.df['AcidIndex'].max() - self.df['AcidIndex'].min()
            )
            self.df['quality_chemistry_interaction'] = self.df['STARS'] * (1 - acid_normalized)
            features_created += 1
            self.log_transformation(
                "Feature Engineering - Quality Chemistry",
                "Created quality_chemistry_interaction (STARS √ó normalized AcidIndex)",
                {'formula': 'STARS * (1 - normalized_AcidIndex)'}
            )
        
        # Alcohol-appeal premium indicator
        if all(col in self.df.columns for col in ['Alcohol', 'LabelAppeal', 'STARS']):
            self.df['alcohol_appeal_premium'] = (
                self.df['Alcohol'] * self.df['LabelAppeal'] * (self.df['STARS'] > 2.5).astype(int)
            )
            features_created += 1
            self.log_transformation(
                "Feature Engineering - Premium Indicator",
                "Created alcohol_appeal_premium (luxury market segment)",
                {'formula': 'Alcohol * LabelAppeal * (STARS > 2.5)'}
            )
        
        print(f"üöÄ Feature engineering completed: {features_created} new features created")
        print(f"üìä New dataset dimensions: {self.df.shape[0]:,} rows √ó {self.df.shape[1]} columns")
        
        return self.df.copy()
    
    def scale_features(self):
        """
        Apply variable-specific scaling strategies.
        
        Strategy:
        - RobustScaler: Heavy outlier variables
        - StandardScaler: Mild outlier variables
        - MinMaxScaler: Bounded variables (STARS)
        - No Scaling: Target variable
        """
        print("\n‚öñÔ∏è  PHASE 5: SCALING AND NORMALIZATION STRATEGY")
        print("=" * 50)
        
        # Define scaling strategies based on strategy document
        robust_scale_vars = [
            'FixedAcidity', 'VolatileAcidity', 'CitricAcid', 'ResidualSugar', 
            'Chlorides', 'FreeSulfurDioxide', 'TotalSulfurDioxide', 'Density', 
            'pH', 'Sulphates', 'Alcohol', 'AcidIndex'
        ]
        standard_scale_vars = ['LabelAppeal']
        minmax_scale_vars = ['STARS']
        no_scale_vars = ['TARGET']
        
        # Include engineered features in robust scaling
        engineered_features = [
            'total_to_free_so2_ratio', 'acidity_balance_ratio', 'sugar_alcohol_ratio',
            'density_alcohol_deviation', 'stars_label_synergy', 'quality_chemistry_interaction',
            'alcohol_appeal_premium'
        ]
        robust_scale_vars.extend([feat for feat in engineered_features if feat in self.df.columns])
        
        # Apply RobustScaler for outlier-heavy variables
        robust_vars_present = [var for var in robust_scale_vars if var in self.df.columns]
        if robust_vars_present:
            scaler = RobustScaler()
            self.df[robust_vars_present] = scaler.fit_transform(self.df[robust_vars_present])
            self.scalers['robust'] = scaler
            
            self.log_transformation(
                "RobustScaler Applied",
                f"Scaled {len(robust_vars_present)} outlier-heavy variables",
                {
                    'method': 'robust_scaler',
                    'variables': robust_vars_present,
                    'rationale': 'Heavy outlier variables from EDA findings'
                }
            )
        
        # Apply StandardScaler for business variables
        standard_vars_present = [var for var in standard_scale_vars if var in self.df.columns]
        if standard_vars_present:
            scaler = StandardScaler()
            self.df[standard_vars_present] = scaler.fit_transform(self.df[standard_vars_present])
            self.scalers['standard'] = scaler
            
            self.log_transformation(
                "StandardScaler Applied",
                f"Scaled {len(standard_vars_present)} business variables",
                {
                    'method': 'standard_scaler',
                    'variables': standard_vars_present,
                    'rationale': 'Important business variables with mild outliers'
                }
            )
        
        # Apply MinMaxScaler for bounded variables
        minmax_vars_present = [var for var in minmax_scale_vars if var in self.df.columns]
        if minmax_vars_present:
            scaler = MinMaxScaler()
            self.df[minmax_vars_present] = scaler.fit_transform(self.df[minmax_vars_present])
            self.scalers['minmax'] = scaler
            
            self.log_transformation(
                "MinMaxScaler Applied",
                f"Scaled {len(minmax_vars_present)} bounded variables",
                {
                    'method': 'minmax_scaler',
                    'variables': minmax_vars_present,
                    'rationale': 'Natural bounded scale (1-4 stars)'
                }
            )
        
        # Log no-scaling variables
        no_scale_present = [var for var in no_scale_vars if var in self.df.columns]
        if no_scale_present:
            self.log_transformation(
                "No Scaling Applied",
                f"Preserved original scale for {len(no_scale_present)} variables",
                {
                    'method': 'no_scaling',
                    'variables': no_scale_present,
                    'rationale': 'Target variable - preserve interpretability'
                }
            )
        
        total_scaled = len(robust_vars_present) + len(standard_vars_present) + len(minmax_vars_present)
        print(f"üìä Scaling summary:")
        print(f"   ‚Ä¢ RobustScaler: {len(robust_vars_present)} variables")
        print(f"   ‚Ä¢ StandardScaler: {len(standard_vars_present)} variables")
        print(f"   ‚Ä¢ MinMaxScaler: {len(minmax_vars_present)} variables")
        print(f"   ‚Ä¢ No scaling: {len(no_scale_present)} variables")
        print(f"   ‚Ä¢ Total scaled: {total_scaled} variables")
        
        return self.df.copy()
    
    def validate_transformations(self):
        """
        Comprehensive validation of all transformations.
        Statistical tests, business validation, and improvement metrics.
        """
        print("\nüî¨ PHASE 6: COMPREHENSIVE VALIDATION")
        print("=" * 50)
        
        validation_results = {
            'normality_improvement': {},
            'missing_data_recovery': {},
            'outlier_impact_reduction': {},
            'correlation_preservation': {},
            'business_validation': {}
        }
        
        # 1. Normality improvement validation
        print("Validating normality improvements...")
        
        numeric_vars = self.df.select_dtypes(include=[np.number]).columns
        exclude_vars = ['TARGET']  # Don't test target variable
        test_vars = [col for col in numeric_vars if col not in exclude_vars]
        
        normality_improved = 0
        for var in test_vars:
            if var not in self.df.columns:
                continue
                
            # Test current normality
            try:
                _, p_value = normaltest(self.df[var].dropna())
                is_normal = p_value > 0.05
                
                validation_results['normality_improvement'][var] = {
                    'p_value': p_value,
                    'is_normal': is_normal,
                    'improvement': 'tested'  # Would need original data for true comparison
                }
                
                if is_normal:
                    normality_improved += 1
                    
            except Exception as e:
                validation_results['normality_improvement'][var] = {
                    'error': str(e)
                }
        
        print(f"   üìà Variables achieving normality: {normality_improved}/{len(test_vars)}")
        
        # 2. Missing data recovery validation
        print("Validating missing data recovery...")
        
        missing_after = self.df.isnull().sum().sum()
        total_cells = len(self.df) * len(self.df.columns)
        missing_rate = missing_after / total_cells * 100
        
        validation_results['missing_data_recovery'] = {
            'total_missing_cells': missing_after,
            'missing_rate_percent': missing_rate,
            'complete_recovery': missing_after == 0
        }
        
        print(f"   üìä Remaining missing data: {missing_after:,} cells ({missing_rate:.2f}%)")
        
        # 3. STARS-TARGET correlation preservation
        print("Validating business relationship preservation...")
        
        if all(col in self.df.columns for col in ['STARS', 'TARGET']):
            stars_target_corr = self.df['STARS'].corr(self.df['TARGET'])
            validation_results['correlation_preservation']['STARS_TARGET'] = {
                'correlation': stars_target_corr,
                'preserved': abs(stars_target_corr) > 0.5  # Should remain strong
            }
            print(f"   üåü STARS-TARGET correlation: {stars_target_corr:.3f}")
        
        # 4. Feature engineering validation
        print("Validating engineered features...")
        
        engineered_features = [
            'total_to_free_so2_ratio', 'acidity_balance_ratio', 'sugar_alcohol_ratio',
            'density_alcohol_deviation', 'stars_label_synergy', 'quality_chemistry_interaction',
            'alcohol_appeal_premium'
        ]
        
        valid_engineered = 0
        for feature in engineered_features:
            if feature in self.df.columns:
                # Check for reasonable values (no infinite/NaN after engineering)
                has_infinite = np.isinf(self.df[feature]).any()
                has_nan = self.df[feature].isnull().any()
                is_valid = not (has_infinite or has_nan)
                
                if is_valid:
                    valid_engineered += 1
                
                validation_results['business_validation'][feature] = {
                    'has_infinite_values': has_infinite,
                    'has_missing_values': has_nan,
                    'is_valid': is_valid
                }
        
        print(f"   üöÄ Valid engineered features: {valid_engineered}/{len([f for f in engineered_features if f in self.df.columns])}")
        
        # 5. Data quality summary
        print("\nData quality summary:")
        total_observations = len(self.df)
        total_variables = len(self.df.columns)
        
        print(f"   üìä Final dataset: {total_observations:,} rows √ó {total_variables} columns")
        print(f"   üéØ Complete cases: {(~self.df.isnull().any(axis=1)).sum():,} ({(~self.df.isnull().any(axis=1)).mean()*100:.1f}%)")
        print(f"   üî¢ Numeric variables: {len(self.df.select_dtypes(include=[np.number]).columns)}")
        print(f"   üìà Engineered features: {len([f for f in engineered_features if f in self.df.columns])}")
        
        self.validation_results = validation_results
        return validation_results
    
    def create_before_after_comparison(self):
        """Create comprehensive before/after comparison visualizations."""
        print("\nüìä Creating before/after comparison visualizations...")
        
        # Select key variables for comparison
        comparison_vars = ['STARS', 'LabelAppeal', 'AcidIndex', 'Alcohol', 'pH']
        comparison_vars = [var for var in comparison_vars if var in self.df.columns]
        
        if not comparison_vars:
            print("No suitable variables found for comparison")
            return
        
        # Create comparison plots
        fig, axes = plt.subplots(len(comparison_vars), 2, figsize=(15, 4*len(comparison_vars)))
        if len(comparison_vars) == 1:
            axes = axes.reshape(1, -1)
        
        for i, var in enumerate(comparison_vars):
            # Before (original) distribution
            if var in self.original_df.columns:
                axes[i, 0].hist(self.original_df[var].dropna(), bins=50, alpha=0.7, color='red', edgecolor='black')
                axes[i, 0].set_title(f'{var} - Before Cleaning')
                axes[i, 0].set_ylabel('Frequency')
                
                # After (cleaned) distribution  
                axes[i, 1].hist(self.df[var].dropna(), bins=50, alpha=0.7, color='green', edgecolor='black')
                axes[i, 1].set_title(f'{var} - After Cleaning')
                axes[i, 1].set_ylabel('Frequency')
        
        plt.tight_layout()
        plt.savefig(f'{self.output_dir}/plots/before_after_distributions.png', dpi=300, bbox_inches='tight')
        plt.close()
        
        print(f"   üìÅ Saved: before_after_distributions.png")
    
    def generate_cleaning_report(self):
        """Generate comprehensive cleaning report."""
        print("\nüìã GENERATING COMPREHENSIVE CLEANING REPORT")
        print("=" * 50)
        
        report = {
            'cleaning_summary': {
                'original_shape': self.original_df.shape,
                'final_shape': self.df.shape,
                'features_added': self.df.shape[1] - self.original_df.shape[1],
                'total_transformations': len(self.transformation_log)
            },
            'transformation_log': self.transformation_log,
            'validation_results': self.validation_results,
            'scalers_used': list(self.scalers.keys()),
            'transformers_used': list(self.transformers.keys()) if hasattr(self, 'transformers') else []
        }
        
        # Save detailed report
        with open(f'{self.output_dir}/cleaning_report.json', 'w') as f:
            json.dump(report, f, indent=2, default=str)
        
        # Create summary report
        summary_lines = [
            "="*70,
            "DATA CLEANING PIPELINE EXECUTION REPORT",
            "="*70,
            "",
            "üîß TRANSFORMATION SUMMARY:",
            f"   ‚Ä¢ Original dataset: {self.original_df.shape[0]:,} rows √ó {self.original_df.shape[1]} columns",
            f"   ‚Ä¢ Final dataset: {self.df.shape[0]:,} rows √ó {self.df.shape[1]} columns",
            f"   ‚Ä¢ Features added: {self.df.shape[1] - self.original_df.shape[1]}",
            f"   ‚Ä¢ Total transformations: {len(self.transformation_log)}",
            "",
            "üìä PHASE COMPLETION STATUS:",
        ]
        
        phases = [
            ("Phase 1", "Missing Data Treatment", "‚úÖ"),
            ("Phase 2", "Outlier Handling", "‚úÖ"), 
            ("Phase 3", "Distribution Transformation", "‚úÖ"),
            ("Phase 4", "Feature Engineering", "‚úÖ"),
            ("Phase 5", "Scaling & Normalization", "‚úÖ"),
            ("Phase 6", "Validation & Testing", "‚úÖ")
        ]
        
        for phase, description, status in phases:
            summary_lines.append(f"   {status} {phase}: {description}")
        
        summary_lines.extend([
            "",
            "üéØ KEY ACHIEVEMENTS:",
            f"   ‚Ä¢ Missing data eliminated: {self.original_df.isnull().sum().sum():,} ‚Üí {self.df.isnull().sum().sum():,}",
            f"   ‚Ä¢ Outliers treated systematically across all numeric variables",
            f"   ‚Ä¢ All non-normal distributions transformed",
            f"   ‚Ä¢ {self.df.shape[1] - self.original_df.shape[1]} domain-specific features engineered",
            f"   ‚Ä¢ Variable-specific scaling applied",
            "",
            "üìÅ DELIVERABLES:",
            f"   ‚Ä¢ Cleaned dataset: {self.output_dir}/M3_Data_cleaned.csv",
            f"   ‚Ä¢ Transformation log: {self.output_dir}/cleaning_report.json",
            f"   ‚Ä¢ Before/after comparisons: {self.output_dir}/plots/",
            "",
            "‚úÖ DATASET READY FOR MACHINE LEARNING PIPELINE",
            "="*70
        ])
        
        summary_text = "\n".join(summary_lines)
        
        # Save summary report
        with open(f'{self.output_dir}/cleaning_summary.txt', 'w') as f:
            f.write(summary_text)
        
        print(summary_text)
        
        return report

def apply_complete_cleaning_pipeline(file_path='M3_Data.csv', output_dir='data_cleaning_results'):
    """
    Execute the complete data cleaning pipeline.
    
    Parameters:
    -----------
    file_path : str
        Path to the input dataset
    output_dir : str  
        Directory for output files and reports
        
    Returns:
    --------
    tuple: (cleaned_df, cleaning_report, cleaner_object)
    """
    print("üöÄ LAUNCHING COMPLETE DATA CLEANING PIPELINE")
    print("=" * 70)
    
    # Load original data
    df = pd.read_csv(file_path, encoding='utf-8-sig')
    print(f"üìä Loaded dataset: {df.shape[0]:,} rows √ó {df.shape[1]} columns")
    
    # Initialize cleaner
    cleaner = ComprehensiveDataCleaner(df, output_dir)
    
    # Execute all phases
    try:
        # Phase 1: Missing data treatment
        cleaner.apply_missing_data_treatment()
        
        # Phase 2: Outlier handling
        cleaner.handle_outliers_comprehensive()
        
        # Phase 3: Distribution transformations
        cleaner.transform_distributions()
        
        # Phase 4: Feature engineering
        cleaner.create_engineered_features()
        
        # Phase 5: Scaling
        cleaner.scale_features()
        
        # Phase 6: Validation
        validation_results = cleaner.validate_transformations()
        
        # Create comparisons and reports
        cleaner.create_before_after_comparison()
        cleaning_report = cleaner.generate_cleaning_report()
        
        # Save cleaned dataset
        cleaned_file_path = f'{output_dir}/M3_Data_cleaned.csv'
        cleaner.df.to_csv(cleaned_file_path, index=False)
        print(f"\nüíæ Cleaned dataset saved: {cleaned_file_path}")
        
        return cleaner.df, cleaning_report, cleaner
        
    except Exception as e:
        print(f"‚ùå Error during cleaning pipeline: {str(e)}")
        raise

if __name__ == "__main__":
    # Execute complete cleaning pipeline
    cleaned_df, report, cleaner = apply_complete_cleaning_pipeline()